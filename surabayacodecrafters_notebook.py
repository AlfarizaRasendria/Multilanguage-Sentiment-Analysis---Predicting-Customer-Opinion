# -*- coding: utf-8 -*-
"""SurabayaCodeCrafters_Notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-7JwFFYQX3lpCvLCpSf5f1DWco1bhuIm

#**ML Olympiad - Know Your Customer Opinion**
***
###**Team Name** : Surabaya Code Crafters
###**Member List** :
###-Rizfi Ferdiansyah
###-Muhammad Alfariza Rasendria
***
###**Email** :
###rizfiferdian@gmail.com
###rasendria.alfariza18@gmail.com

# **Outline**
1. **Import Library**
2. **Augmentation Dataset based on balancing Language distribution and number of Dataset record each label**
3. **Normalized Train Data**
4. **Train RNN Model**
5. **Normalized Test Data**
6. **Predict Test Data**

### **Import The Required Libraries**
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
import zipfile,os
!pip install deep_translator --quiet
from deep_translator import GoogleTranslator
from sklearn.model_selection import train_test_split
import re
import string

!pip install kaggle --quiet
!pip install nlp_id --quiet
!pip install nltk --q
import nltk
nltk.download('wordnet')
nltk.download('punkt')
nltk.download('stopwords')

from google.colab import drive
drive.mount('/content/drive')

import os
os.environ['KAGGLE_CONFIG_DIR'] = '/content/drive/MyDrive/Kaggle'

!kaggle competitions download -c ml-olympiad-tfugsurabaya-2024

file_path = '/content/ml-olympiad-tfugsurabaya-2024.zip'

with zipfile.ZipFile(file_path, 'r') as zip_ref:
    zip_ref.extractall('/tmp/')
    zip_ref.close()

base_dir = '/tmp'

df_train = pd.read_csv(f'{base_dir}/train.tsv',sep = "\t")
df_train.head(10)

df_train.shape

df_train.head(10)

!pip install langdetect
from langdetect import detect

def detect_lang(text):
  try:
    return detect(text)
  except:
    return "unknown"

df_train['Language'] = df_train['REVIEW'].apply(detect_lang)

print(df_train)

import matplotlib.pyplot as plt
class_distribution = df_train['Language'].value_counts()
print("Distribusi language:")
print(class_distribution)
print(len(class_distribution))

plt.figure(figsize=(10, 6))
plt.bar(class_distribution.index, class_distribution.values, color='orange')
plt.xlabel('Kelas')
plt.ylabel('Jumlah Sampel')
plt.title('Dataset Language Distribution')
plt.xticks(class_distribution.index,)
plt.show()

def replaceInaccurateLanguage(dataset=df_train):
    for index, row in dataset.iterrows():
        if row['Language'] not in ['id', 'en']:
            dataset.at[index, 'Language'] = 'id'
    return dataset

df_train = replaceInaccurateLanguage()
print(df_train)

import matplotlib.pyplot as plt
class_distribution = df_train['Language'].value_counts()
print("Distribusi language:")
print(class_distribution)
print(len(class_distribution))

plt.figure(figsize=(10, 6))
plt.bar(class_distribution.index, class_distribution.values, color='orange')
plt.xlabel('Kelas')
plt.ylabel('Jumlah Sampel')
plt.title('Dataset Language Distribution')
plt.xticks(class_distribution.index,)
plt.show()

indonesian_records = df_train.loc[df_train['Language'] == 'id']
english_records  = df_train.loc[df_train['Language'] == 'en']

!pip install textaugment --q

!pip show textaugment

!pip freeze

!pip show gensim

!cat /usr/local/lib/python3.10/dist-packages/textaugment/translate.py

!pip install textaugment --q
from textaugment import Translate

def indonesian_augment(dataset, columns=[2,3]):
    new_data = []
    t = Translate(src="id", to="en")

    for index, row in dataset.iterrows():
        try:
            if row['LABEL'] == columns[0] and row['Language'] == 'id':
                augmented_indonesian_review = t.augment(row['REVIEW'])
                print(f'augmented indonesian result : {augmented_indonesian_review}')
                new_data.append({
                    'ID': row.ID,
                    'REVIEW': augmented_indonesian_review,
                    'Language': 'id',
                    'LABEL': row.LABEL
                })
            elif row['LABEL'] == columns[1] and row['Language'] == 'id':
                augmented_indonesian_review = t.augment(row['REVIEW'])
                print(f'augmented indonesian result : {augmented_indonesian_review}')
                new_data.append({
                    'ID': row.ID,
                    'REVIEW': augmented_indonesian_review,
                    'Language': 'id',
                    'LABEL': row.LABEL
                })

        except Exception as e:
            print(f'Error at {e}')
            continue

    new_data = pd.DataFrame(new_data)
    dataset = pd.concat([dataset, new_data], ignore_index=True)
    return dataset

df_train = indonesian_augment(df_train)
print(df_train)

import matplotlib.pyplot as plt
class_distribution = df_train['Language'].value_counts()
print("Distribusi language:")
print(class_distribution)
print(len(class_distribution))

plt.figure(figsize=(10, 6))
plt.bar(class_distribution.index, class_distribution.values, color='orange')
plt.xlabel('Kelas')
plt.ylabel('Jumlah Sampel')
plt.title('Dataset Language Distribution')
plt.xticks(class_distribution.index,)
plt.show()



df_train = augmentDataByTranslate(df_train,indonesian_records)
print(df_train)

print(df_train)

import matplotlib.pyplot as plt
class_distribution = df_train['Language'].value_counts()
print("Distribusi language:")
print(class_distribution)
print(len(class_distribution))

plt.figure(figsize=(10, 6))
plt.bar(class_distribution.index, class_distribution.values, color='orange')
plt.xlabel('Kelas')
plt.ylabel('Jumlah Sampel')
plt.title('Dataset Language Distribution')
plt.xticks(class_distribution.index,)
plt.show()


import matplotlib.pyplot as plt

labels = ['1', '2', '3', '4', '5']
values = range(1, 6)

class_distribution = df_train['LABEL'].value_counts()

plt.figure(figsize=(10, 6))
plt.bar(class_distribution.index, class_distribution.values, color=['orange', 'cyan', 'green', 'red', 'blue'])
plt.xlabel('Kelas')
plt.ylabel('Jumlah Sampel')
plt.title('Distribusi Kelas yang ada pada Dataset')
plt.xticks(values, labels)
plt.show()

!pip install textattack
from textattack.augmentation import EasyDataAugmenter
import pandas as pd

def augmentedEachMinority(label_arr, class_distribution, dataset=df_train):
    majority_class_record = class_distribution.max()
    print(f"jumlah record pada label mayoritas yaitu {majority_class_record}")
    aug = EasyDataAugmenter()

    new_data = []

    for label in label_arr:
        record_count = class_distribution[label]

        row_each_label = dataset[dataset['LABEL'] == label]

        for index, row in row_each_label.iterrows():
            if record_count <= majority_class_record and row['Language'] == 'en':
                try:
                    print(f"Jumlah record count pada label {label} saat ini yaitu sebesar {record_count}")
                    augmented_review = aug.augment(row['REVIEW'])
                    print(f'augmented_result : {augmented_review}')
                    augmented_english_length = len(augmented_review)
                    for i in range(augmented_english_length):
                        new_data.append({
                            'ID': row.ID + len(dataset) - 1 + i,
                            'REVIEW': augmented_review[i],
                            'Language': 'en',
                            'LABEL': row.LABEL
                        })
                        record_count += 1

                        translated_augmented = GoogleTranslator(source="auto", target='id').translate(augmented_review[i])
                        print(f'Translated to Indonesia : {translated_augmented}')
                        translated_review = translated_augmented
                        new_data.append({
                            'ID': row.ID + len(dataset) - 1 + i,
                            'REVIEW': translated_review,
                            'Language': 'id',
                            'LABEL': row.LABEL
                        })
                        record_count += 1

                except Exception as e:
                    print(f'Error at {e}')
                    continue

    new_df = pd.DataFrame(new_data)
    dataset = pd.concat([dataset, new_df], ignore_index=True)

    return dataset

df_train = augmentedEachMinority([1,2,3,4],class_distribution)

print(df_train)

print(len(df_train))

import matplotlib.pyplot as plt

labels = ['1', '2', '3', '4', '5']
values = range(1, 6)

class_distribution = df_train['LABEL'].value_counts()

plt.figure(figsize=(10, 6))
plt.bar(class_distribution.index, class_distribution.values, color=['orange', 'cyan', 'green', 'red', 'blue'])
plt.xlabel('Kelas')
plt.ylabel('Jumlah Sampel')
plt.title('Distribusi Kelas yang ada pada Dataset')
plt.xticks(values, labels)
plt.show()

from nltk.stem import WordNetLemmatizer
from nlp_id.lemmatizer import Lemmatizer
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords



from sklearn.preprocessing import OneHotEncoder

enc = OneHotEncoder(handle_unknown='ignore')

enc_df = pd.DataFrame(enc.fit_transform(df_train[['LABEL']]).toarray())

df_train = df_train.join(enc_df)

df_train

df_train['REVIEW'] = normalize_text(df_train)
print()
print("RESULTTT ============")
print(df_train.head())

import pandas as pd

if df_train['REVIEW'].isnull().any():
    df_train.dropna(subset=['REVIEW'], inplace=True)

sentiment = df_train['REVIEW'].values
label = df_train[[0,1,2,3,4]].values

def get_corpus(text):
    words = []
    for i in text:
        for j in str(i).split():
            words.append(j.strip())
    return words

corpus = get_corpus(df_train.REVIEW)
corpus

from collections import Counter
counter = Counter(corpus)
counter

from sklearn.model_selection import train_test_split
trained_sentiment, tested_sentiment, trained_label, tested_label = train_test_split(df_train.REVIEW,label,test_size=0.2)

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

tokenizer = Tokenizer(num_words=5000, oov_token='<oov>')
tokenizer.fit_on_texts(trained_sentiment)

trained_sequences = tokenizer.texts_to_sequences(trained_sentiment)
tested_sequences = tokenizer.texts_to_sequences(tested_sentiment)

trained_padded = pad_sequences(trained_sequences,padding='post',
maxlen=50,
truncating='post')
tested_padded = pad_sequences(tested_sequences,padding='post',
maxlen=50,
truncating='post')

import tensorflow as tf

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=5000, output_dim=64, input_length=50),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)),
    tf.keras.layers.Dropout(0.4),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dropout(0.4),
    tf.keras.layers.Dense(64, kernel_regularizer=tf.keras.regularizers.l2(0.01)),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dropout(0.4),
    tf.keras.layers.Dense(5, activation='softmax')
])

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()

class custom_Callback_class(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy') > 0.95 and logs.get('val_accuracy') > 0.85) :
      print("\nProses Training Dihentikan karena Akurasi telah melampaui 95% dan validation accuracy telah melampaui 85%")
      self.model.stop_training = True

custom_Callback = custom_Callback_class()

from sklearn.utils.class_weight import compute_class_weight
def count_class_weights(class_series):

  class_labels = np.unique(class_series)
  class_weights = compute_class_weight(class_weight='balanced', classes=class_labels, y=class_series)
  return dict(zip(class_labels, class_weights))

class_weights = count_class_weights([0, 1, 2, 3, 4])

num_epochs = 30
history_model = model.fit(trained_padded, trained_label, epochs=num_epochs,
                    validation_data=(tested_padded, tested_label), verbose=2, callbacks = [custom_Callback],class_weight=class_weights)

import matplotlib.pyplot as plt
plt.plot(history_model.history['accuracy'], color='green', label='Train Accuracy')
plt.plot(history_model.history['val_accuracy'], color='yellow', label='Validation Accuracy')
plt.title('Akurasi Model')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['Accuracy', 'Validation Accuracy'], loc='upper left')
plt.show()

plt.plot(history_model.history['loss'], color='red', label='Train Loss')
plt.plot(history_model.history['val_loss'], color='orange', label='Validation Loss')
plt.title('Loss Model')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['Train Loss', 'Validation Loss'], loc='upper right')
plt.show()

"""Reading Test Set"""

df_test =pd.read_csv(f'{base_dir}/test.tsv',sep = "\t")

df_test["Language"] = df_test['REVIEW'].apply(detect_lang)

def normalize_test_text(input_df):
    result = []
    for index, row in input_df.iterrows():
        input_text = row.REVIEW
        lang = row.Language
        stopwords_lang = 'english' if lang == 'en' else 'indonesian'
        lowercase = tf.strings.lower(input_text)
        stripped_string = tf.strings.regex_replace(lowercase,
                                               '[%s]' % re.escape(string.punctuation),
                                               '')
        stripped_string_list = stripped_string.numpy()
        words = stripped_string_list.split()

        lemmatizer = WordNetLemmatizer() if lang == 'en' else Lemmatizer()

        normalized_text = " ".join([lemmatizer.lemmatize(word.decode('utf-8')) for word in words])
        normalized_text = re.sub(r'(.)\1{2,}', r'\1', normalized_text)
        tokenized_words = word_tokenize(normalized_text)

        stop_words = stopwords.words(stopwords_lang)
        tokenized_words =  [word for word in tokenized_words if not word.lower() in stop_words]

        sentence = " ".join(tokenized_words)
        result.append(sentence)

    return result

df_test['REVIEW'] = normalize_test_text(df_test)

tokenizer = Tokenizer(num_words=5000, oov_token='<oov>')
tokenizer.fit_on_texts(df_test.REVIEW)

tested_sequences = tokenizer.texts_to_sequences(df_test.REVIEW)

tested_padded = pad_sequences(tested_sequences,padding='post',
maxlen=50,
truncating='post')



df_test = df_test.drop(['REVIEW','Language'], axis=1)

from google.colab import files
df_test.to_csv('submission.csv', index=False)
files.download('submission.csv')